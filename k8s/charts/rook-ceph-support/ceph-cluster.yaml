kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: my-cluster
  namespace: rook-ceph # namespace:cluster
spec:
  # If there are multiple clusters, the directory must be unique for each cluster.
  dataDirHostPath: /var/lib/rook
  mon:
    count: 1
    allowMultiplePerNode: true
    volumeClaimTemplate:
      spec:
        storageClassName: local-storage
        resources:
          requests:
            storage: 1Gi
  mgr:
    count: 1
    modules:
      - name: rook
        enabled: true
  dashboard:
    enabled: true
    ssl: false
  crashCollector:
    disable: false
  cephVersion:
    image: quay.io/ceph/ceph:v18
    allowUnsupported: false
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  storage:
    storageClassDeviceSets:
      - name: set1
        count: 3
        portable: false
        tuneDeviceClass: true
        tuneFastDeviceClass: false
        encrypted: false
        placement:
        preparePlacement:
        volumeClaimTemplates:
          - metadata:
              name: data
              # if you are looking at giving your OSD a different CRUSH device class than the one detected by Ceph
              # annotations:
              #   crushDeviceClass: hybrid
            spec:
              resources:
                requests:
                  storage: 20Gi
              # IMPORTANT: Change the storage class depending on your environment
              storageClassName: local-storage
              volumeMode: Block
              accessModes:
                - ReadWriteOnce
    # when onlyApplyOSDPlacement is false, will merge both placement.All() and storageClassDeviceSets.Placement
    onlyApplyOSDPlacement: false
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0
  cephConfig:
    global:
      mon_warn_on_pool_no_redundancy: "false"
---
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: builtin-mgr
  namespace: rook-ceph # namespace:cluster
spec:
  name: .mgr
  failureDomain: osd
  replicated:
    size: 1
    requireSafeReplicaSize: false

---
# PV for Monitor (needs Filesystem mode)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-mon
spec:
  storageClassName: local-storage
  capacity:
    storage: 2Gi  # Must match or exceed mon request
  volumeMode: Filesystem  # Important for monitor!
  accessModes:
    - ReadWriteOnce
  local:
    path: /dev/sda1
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - nixos-1  # Must match your node name exactly

---
# PV for OSD (needs Block mode)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-osd
spec:
  storageClassName: local-storage
  capacity:
    storage: 26Gi  # Must match or exceed OSD request
  volumeMode: Block  # Important for OSD!
  accessModes:
    - ReadWriteOnce
  local:
    path: /dev/sda2
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - nixos-1