apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-cluster
  namespace: argocd
spec:
  project: default
  destination:
    server: https://kubernetes.default.svc
    namespace: rook-ceph
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
  sources:
    - path: k8s/charts/support
      repoURL: https://github.com/yurifrl/home-systems.git
      targetRevision: HEAD
      helm:
        valuesObject:
          virtualServices:
            - name: ceph
              service:
                name: rook-ceph-mgr-dashboard
                namespace: rook-ceph
                port: 7000
    - path: k8s/charts/rook-ceph-support
      repoURL: https://github.com/yurifrl/home-systems.git
      targetRevision: HEAD
      directory:
        recurse: true
    - chart: rook-ceph-cluster
      repoURL: https://charts.rook.io/release
      targetRevision: "*"
      helm:
        valuesObject:
          toolbox:
            enabled: true
          monitoring:
            enabled: true
            createPrometheusRules: true
          cephClusterSpec:
            skipUpgradeChecks: false
            continueUpgradeAfterChecksEvenIfNotHealthy: false
            dataDirHostPath: /var/lib/rook
            dashboard:
              enabled: true
              ssl: false
            crashCollector:
              disable: false
            cephVersion:
              image: quay.io/ceph/ceph:v18
              allowUnsupported: false

            mgr:
              count: 1
              modules:
                - name: rook
                  enabled: true   
            mon:
              count: 1
              allowMultiplePerNode: true
              volumeClaimTemplate:
                spec:
                  storageClassName: my-storage-class
                  resources:
                    requests:
                      storage: 5Gi
            storage:
              useAllNodes: false
              nodes:
                - name: "nixos-1"
                  devices:
                    - name: "/dev/sda2"
                  config:
                    metadataDevice: "/dev/sda1"
              storageClassDeviceSets:
                - name: set1
                  count: 1
                  portable: false
                  tuneDeviceClass: true
                  tuneFastDeviceClass: false
                  encrypted: false
                  placement: {}
                  preparePlacement: {}
                  volumeClaimTemplates:
                    - metadata:
                        name: data
                        # if you are looking at giving your OSD a different CRUSH device class than the one detected by Ceph
                        # annotations:
                        #   crushDeviceClass: hybrid
                      spec:
                        resources:
                          requests:
                            storage: 5Gi
                        # IMPORTANT: Change the storage class depending on your environment
                        storageClassName: my-storage-class
                        volumeMode: Block
                        accessModes:
                          - ReadWriteOnce
              # when onlyApplyOSDPlacement is false, will merge both placement.All() and storageClassDeviceSets.Placement
              onlyApplyOSDPlacement: false
            priorityClassNames:
              mon: system-node-critical
              osd: system-node-critical
              mgr: system-cluster-critical
            disruptionManagement:
              managePodBudgets: true
              osdMaintenanceTimeout: 30
              pgHealthCheckTimeout: 0
            cephConfig:
              global:
                mon_warn_on_pool_no_redundancy: "false"                    