# https://github.com/rook/rook/blob/master/deploy/charts/rook-ceph/values.yaml
# https://github.com/search?q=csiCephFSPluginVolume+nix+language%3AYAML&type=code&l=YAML
# https://github.com/annie444/k3s-cluster/blob/main/core/storage/rook-ceph/cluster/helm-release.yaml
# https://github.com/Ramblurr/home-ops/blob/main/k8s/prod/core/rook-ceph/rook-ceph/app/helmrelease.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-cluster
  namespace: argocd
spec:
  project: default
  destination:
    server: https://kubernetes.default.svc
    namespace: rook-ceph
  syncPolicy:
    automated:
      prune: true
      selfHeal: false
    syncOptions:
      - CreateNamespace=true
  sources:
    - path: k8s/charts/support
      repoURL: https://github.com/yurifrl/home-systems.git
      targetRevision: HEAD
      helm:
        valuesObject:
          virtualServices:
            - name: ceph
              service:
                name: rook-ceph-mgr-dashboard
                namespace: rook-ceph
                port: 8443
    - path: k8s/charts/rook-ceph-support
      repoURL: https://github.com/yurifrl/home-systems.git
      targetRevision: HEAD
      directory:
        recurse: true
    - chart: rook-ceph-cluster
      repoURL: https://charts.rook.io/release
      targetRevision: "*"
      helm:
        valuesObject:
          toolbox:
            enabled: true
          monitoring:
            enabled: true
            createPrometheusRules: true
          cephClusterSpec:
            # If there are multiple clusters, the directory must be unique for each cluster.
            dataDirHostPath: /var/lib/rook
            mon:
              count: 1
              allowMultiplePerNode: true
            mgr:
              count: 1
            dashboard:
              enabled: true
            storage:
              useAllNodes: true
              useAllDevices: true
            resources:
              mgr:
                limits:
                  cpu: "250m"        # Halved from 500m
                  memory: "512Mi"    # Halved from 1Gi
                requests:
                  cpu: "125m"        # Halved from 250m
                  memory: "256Mi"    # Halved from 512Mi
              mon:
                limits:
                  cpu: "250m"        # Halved from 500m
                  memory: "512Mi"    # Halved from 1Gi
                requests:
                  cpu: "125m"        # Halved from 250m
                  memory: "256Mi"    # Halved from 512Mi
              osd:
                limits:
                  cpu: "250m"        # Halved from 500m
                  memory: "512Mi"    # Halved from 1Gi
                requests:
                  cpu: "125m"        # Halved from 250m
                  memory: "256Mi"    # Halved from 512Mi
          cephFileSystems:
            - name: ceph-filesystem
              # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
              spec:
                metadataPool:
                  replicated:
                    size: 3
                dataPools:
                  - failureDomain: host
                    replicated:
                      size: 3
                    # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
                    name: data0
                metadataServer:
                  activeCount: 1
                  activeStandby: true
                  resources: {}
                    # limits:
                    #   cpu: "2000m"    # Limit to 2 CPU cores
                    #   memory: "2Gi"   # Limit to 2Gi memory
                    # requests:
                    #   cpu: "1000m"    # Request 1 CPU core
                    #   memory: "1Gi"   # Request 1Gi memory
                  priorityClassName: system-cluster-critical
              storageClass:
                enabled: true
                isDefault: false
                name: ceph-filesystem
                # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
                pool: data0
                reclaimPolicy: Delete
                allowVolumeExpansion: true
                volumeBindingMode: "Immediate"
                annotations: {}
                labels: {}
                mountOptions: []
                # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
                parameters:
                  # The secrets contain Ceph admin credentials.
                  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
                  csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
                  csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
                  csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
                  # Specify the filesystem type of the volume. If not specified, csi-provisioner
                  # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
                  # in hyperconverged settings where the volume is mounted on the same node as the osds.
                  csi.storage.k8s.io/fstype: ext4